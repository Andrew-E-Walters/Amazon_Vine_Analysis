{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.1.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"M16-Amazon-Challenge\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Sports_v1_00.tsv.gz\"), sep=\"\\t\", header=True, inferSchema=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "# Read in the Review dataset as a DataFrame\n",
    "Review_Dataset = df\n",
    "Review_Dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the customers_table DataFrame\n",
    "customers_table = df.groupby(\"customer_id\").agg({\"customer_id\":'count'}).withColumnRenamed(\"count(customer_id)\",\"customer_count\")\n",
    "customers_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the products_table DataFrame and drop duplicates. \n",
    "products_table = df.select([\"product_id\", \"product_title\"]).drop_duplicates([\"product_id\"])\n",
    "products_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "# Create the review_id_table DataFrame. \n",
    "# Convert the 'review_date' column to a date datatype with to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")\n",
    "review_id_df = df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\",\"review_date\", to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")])\n",
    "review_id_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vine_table. DataFrame\n",
    "vine_table = df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\",\"vine\", \"verified_purchase\"])\n",
    "vine_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2_df = vine_table.filter(\"total_votes>=20\")\n",
    "D2_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "D2_Percent_Helpful = D2_df.withColumn('percent_helpful_votes',D2_df['helpful_votes']/D2_df['total_votes']).alias('percent_helpful_votes').filter(col(\"percent_helpful_votes\") >= 0.5)\n",
    "D2_Percent_Helpful.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on exact state\n",
    "Vine_Review = D2_Percent_Helpful.filter(D2_Percent_Helpful[\"vine\"] == \"Y\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on exact state\n",
    "Non_Vine_Review = D2_Percent_Helpful.filter(D2_Percent_Helpful[\"vine\"] == \"N\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Reviews = D2_Percent_Helpful.drop_duplicates(subset=['review_id']).count()\n",
    "print(Unique_Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vine_Reviews = D2_Percent_Helpful.filter(D2_Percent_Helpful[\"vine\"] == \"Y\").count()\n",
    "Vine_Reviews_5Star = D2_Percent_Helpful.filter(D2_Percent_Helpful[\"vine\"] == \"Y\").filter(D2_Percent_Helpful[\"star_rating\"] == \"5\").count()\n",
    "Percentage_5_Star = Vine_Reviews_5Star / Vine_Reviews\n",
    "\n",
    "print(Vine_Reviews)\n",
    "print(Vine_Reviews_5Star)\n",
    "print(Percentage_5_Star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Non_Vine_Reviews = D2_Percent_Helpful.filter(D2_Percent_Helpful[\"vine\"] == \"N\").count()\n",
    "Non_Vine_Reviews_5Star = D2_Percent_Helpful.filter(D2_Percent_Helpful[\"vine\"] == \"N\").filter(D2_Percent_Helpful[\"star_rating\"] == \"5\").count()\n",
    "Non_Percentage_5_Star = Non_Vine_Reviews_5Star / Non_Vine_Reviews\n",
    "\n",
    "print(Non_Vine_Reviews)\n",
    "print(Non_Vine_Reviews_5Star)\n",
    "print(Non_Percentage_5_Star)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
